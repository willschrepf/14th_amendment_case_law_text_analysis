---
title: "basic_stats"
author: "Will Schrepferman"
date: "10/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("rjson")
library("tidyjson")
library("glue")
library("skimr")
library("openxlsx")
library("tidytext")
library("SnowballC")
library("rvest")
```

```{r read_data_one_case}
# testing functionality of reading one single json file into a tibble row

json_file <- "school-segregation-cases/1000012.json"

# using rjson package functionality
json_data <- fromJSON(file=json_file)


# these two methods are obtional, but make the data a little uglier
# data_flat <- flatten(json_data)
# data_tbl <- as_tibble(data_flat)

# read only the elements we need from their place in the json object
full_text <- json_data$clusters[[1]]$sub_opinions[[1]]$html_lawbox[1]
id <- json_data$id
resource_uri <- json_data$resource_uri
court_raw <- json_data$court
judges <- json_data$clusters[[1]]$judges[1]
date_filed <- json_data$clusters[[1]]$date_filed[1]
slug <- json_data$clusters[[1]]$slug
federal_cite <- json_data$clusters[[1]]$federal_cite_one[1]


# put items into a tibble row
tibble_data <- tibble(id = id, slug = slug, date_filed = date_filed, court_raw = court_raw, judges = judges, federal_cite = federal_cite,
                      full_text = full_text, resource_uri = resource_uri)
```




```{r read_data_multiple_cases}
# list of all files
files <- list.files("school-segregation-cases")

# function to read one file into a coherent row
readfile <- function(file){
  
  # get the whole file
  json_file <- glue("school-segregation-cases/", file, sep = "")

  # using rjson package functionality
  json_data <- fromJSON(file=json_file)
  
  # these two methods are optional, but make the data a little uglier
  # data_flat <- flatten(json_data)
  # data_tbl <- as_tibble(data_flat)
  
  # read only the elements we need from their place in the json object
  full_text <- json_data$clusters[[1]]$sub_opinions[[1]]$html_lawbox[1]
  id <- json_data$id
  resource_uri <- json_data$resource_uri
  court_raw <- json_data$court
  judges <- json_data$clusters[[1]]$judges[1]
  date_filed <- json_data$clusters[[1]]$date_filed[1]
  slug <- json_data$clusters[[1]]$slug
  federal_cite <- json_data$clusters[[1]]$federal_cite_one[1]
  
  
  # put items into a tibble row
  tibble_data <- tibble(id = id, slug = slug, date_filed = date_filed, court_raw = court_raw, judges = judges, federal_cite = federal_cite,
                      full_text = full_text, resource_uri = resource_uri)
}

# make an empty tibble to put everything in
data_complete <- tibble()


for(i in files){
  # add tibble row from the function onto the final tibble
  data_complete <- rbind(data_complete, readfile(i))
}

tail(data_complete)

# ERROR: running the method above hits a snag when you get to file id 3032772
# will run analysis on the 8287 of 13572 objects that made their way into data frame

skim(data_complete)

head(data_complete, n = 100)

# filter out only cases for which we have all variables

none_missing_data_complete <- data_complete %>%
  filter(judges != "" & federal_cite != "" & full_text != "")

# create a set of cases with at least 1 incomplete field to export for review

incomplete_cases <- data_complete %>%
  filter(judges == "" | federal_cite == "" | full_text == "")

incomplete_cases <- incomplete_cases %>%
  mutate_if(is.character, list(~na_if(.,""))) 

skim(incomplete_cases)
head(incomplete_cases)

write.xlsx(incomplete_cases, "incomplete_cases_export.xlsx")

# 3593 of 8287 (43.4%)

# To-Do:
# Discuss which cases to exclude - DONE
# Resolve error that only gives me half of the data set - TRIED FOR AN HOUR AND COULD NOT RESOLVE
# create a 'document length' variable - DONE
# mapping court_raw to an actual court variable -> go to court listener
# export list of incomplete cases - DONE
# begin stemming + preprocessing text - DONE
```

```{r text_cleaning}
# create testing data with only 25 cases for computing speed purposes

test_data <- head(none_missing_data_complete, 25)

# add variables for year, clean up date type, cut fluff from court type
test_data <- test_data %>%
  mutate(year = as.numeric(substr(date_filed, start = 1, stop = 4))) %>%
  mutate(exact_date = as.Date(date_filed)) %>%
  mutate(court = substr(court_raw, start = 53, stop = (nchar(court_raw)-1))) %>%
  select(id, slug, year, court, judges, federal_cite, full_text, exact_date, resource_uri)

# get an idea of document word counts pre-clean

test_data <- test_data %>%
  mutate(doc_length_pre_clean = sapply(strsplit(full_text, " "), length))

# remove formatting characters. still working on this TODO

#test_data <- test_data %>%
#  mutate(clean_text = str_replace_all(full_text, "[^[:alnum:]]", " ")) %>%
#  mutate(clean_text = str_remove(clean_text, "div")) %>%
#  mutate(clean_text = str_remove(clean_text, "h1")) %>%
#  select(id, slug, year, court, judges, federal_cite, clean_text, exact_date, resource_uri)

# update, it all worked with this silly goose of a function:

strip_html <- function(s) {
    html_text(read_html(s))
}

clean_text <- function(htmlString) {
  return(gsub("<.*?>", "", htmlString))
}

test_data <- test_data %>%
  mutate(clean_text = clean_text(full_text)) %>%
  mutate(doc_length_post_clean = sapply(strsplit(clean_text, " "), length))

# unnesting tokens for one row; this will be turned into a function eventually

test_data_tokens <- test_data %>%
  filter(id == 1000012) %>%
  unnest_tokens(word, clean_text) %>%
  anti_join(get_stopwords()) %>%
  select(word)

# test for most common words. this is very broken, need to get rid of those weird characters.

test_data_tokens %>%
  count(word, sort = TRUE)

# stem words using porter stemming algorithm: https://tartarus.org/martin/PorterStemmer/

test_data_tokens_stemmed <- test_data_tokens %>%
  mutate(stem = wordStem(word)) %>%
  count(stem, sort = TRUE)

# make a lil graph

top_words_graph <- test_data_tokens_stemmed %>%
  top_n(25, n) %>%
  ggplot(aes(n, fct_reorder(stem, n))) +
  geom_col(show.legend = FALSE) +
  labs(x = "Frequency", y = NULL)

# function to do all that for any given id:

get_top_words <- function(id){
  test_data_tokens <- test_data %>%
    filter(id == id) %>%
    unnest_tokens(word, clean_text) %>%
    anti_join(get_stopwords()) %>%
    select(word)

  test_data_tokens %>%
    count(word, sort = TRUE)
  
  test_data_tokens_stemmed <- test_data_tokens %>%
    mutate(stem = wordStem(word)) %>%
    count(stem, sort = TRUE)
  
  top_words_graph <- test_data_tokens_stemmed %>%
    top_n(40, n) %>%
    ggplot(aes(n, fct_reorder(stem, n))) +
    geom_col(show.legend = FALSE) +
    labs(x = "Frequency", y = NULL)
  
  (top_words_graph)
}

get_top_words(id = 1003718)



# BEGIN WORKING ON WORD FREQUENCY SEARCH



```


```{r single_search}

# fix variable names from complete data, select relevant variables

full_corpus <- none_missing_data_complete %>%
  mutate(year = as.numeric(substr(date_filed, start = 1, stop = 4))) %>%
  mutate(exact_date = as.Date(date_filed)) %>%
  mutate(court = substr(court_raw, start = 53, stop = (nchar(court_raw)-1))) %>%
  select(id, slug, year, court, judges, federal_cite, full_text, exact_date, resource_uri)

# corpus_40s is a usefully small corpus of cases from the 40s with ~150K words in total

corpus_40s <- full_corpus %>%
  filter(year >= 1940 & year <= 1949) %>%
  mutate(doc_length_pre_clean = sapply(strsplit(full_text, " "), length)) %>%
  mutate(clean_text = clean_text(full_text)) %>%
  mutate(doc_length_post_clean = sapply(strsplit(clean_text, " "), length)) %>%
  select(id, slug, year, court, judges, federal_cite, clean_text, doc_length_post_clean, exact_date, resource_uri)

# per 12/8 instructions, these are the relevant years
# text is cleaned using clean_text function from previous code chunk

corpus_relevant <- full_corpus %>%
  filter(year >= 1950 & year <= 1974) %>%
  mutate(doc_length_pre_clean = sapply(strsplit(full_text, " "), length)) %>%
  mutate(clean_text = clean_text(full_text)) %>%
  mutate(doc_length_post_clean = sapply(strsplit(clean_text, " "), length)) %>%
  select(id, slug, year, court, judges, federal_cite, clean_text, doc_length_post_clean, exact_date, resource_uri)

# search for the occurences of a single word in a given corpus

search_for_single_word <- function(corpus_for_search, input_word){
  
  num_cases = nrow(corpus_for_search)
  
  # unnest words into long tidy form
  
  tokens <- corpus_for_search %>%
    unnest_tokens(word, clean_text) %>%
    anti_join(get_stopwords()) %>%
    select(word)
  
  # turn words to stems, count occurrences
  
  token_stems <- tokens %>%
    mutate(stem = wordStem(word)) %>%
    count(stem, sort = TRUE)
  
  # get the stem of the input word we want
  
  stem_input_word <- wordStem(input_word)
  
  # see how many times it occurs
  
  num_occurences <- token_stems %>%
    filter(stem == stem_input_word) %>%
    select(n) %>%
    slice(1) %>%
    pull(1)
  
  num_occurences
}

# test out the function

search_for_single_word(corpus_relevant, "diversity")

# function for getting a graph of occurences for an input word

search_for_single_word_graph <- function(corpus_for_search, input_word){
  
  # get a long tibble of all words
  
  tokens_graph <- corpus_for_search %>%
    unnest_tokens(word, clean_text) %>%
    anti_join(get_stopwords()) %>%
    select(word, year)
  
  # stem and count total of all words
  
  token_stems_graph <- tokens_graph %>%
    group_by(year) %>%
    mutate(stem = wordStem(word)) %>%
    count(stem, sort = TRUE)
  
  # get total words per year
  
  yearly_totals <- token_stems_graph %>%
    group_by(year) %>%
    summarize(yearly_total = sum(n))
  
  # stem the word we want
  
  stem_input_word_graph <- wordStem(input_word)
  
  # get a tibble with the yearly occurrences of the word we want
  
  num_occurences_tibble <- token_stems_graph %>%
    filter(stem == stem_input_word_graph)
  
  # get the total occurrences of the word regardless of year
  
  total_num_occurences <- num_occurences_tibble %>%
    ungroup() %>%
    summarise(total = sum(n))
  
  # join occurences of our word and total word occurences by year
  
  num_occurences_data <- right_join(num_occurences_tibble, yearly_totals, by = "year") %>%
    mutate(percent_occurences = (n/yearly_total)) %>%
    select(stem, year, percent_occurences) %>%
    arrange(year)
  
  # replace any blank years as 0
  
  num_occurences_data <- replace_na(num_occurences_data, list(percent_occurences = 0, stem = stem_input_word_graph))
  
  # these two pieces just find the stem to look for for printing purposes
  
  stem_to_look_for_array <- num_occurences_data %>%
    select(stem) %>%
    pull(2)
  
  stem_to_look_for <- stem_to_look_for_array[1]
    
  # graph everything! fairly straightforward
  
  num_occurences_graph <- num_occurences_data %>%
    ggplot(aes(x = year, y = percent_occurences)) +
    geom_line() + 
    scale_x_continuous(breaks = round(seq(min(yearly_totals$year), max(yearly_totals$year), by = 5),1)) +
    scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
    labs(title = paste("Occurences of Stem: '", stem_to_look_for, "' (lemmatized from '", input_word, "') in Given Corpus", sep = ""),
         x = "Year", y = "Percent Occurrences", 
         subtitle = "As a Percentage of Total Words in Corpus for a Given Year",
         caption = paste("There were ", total_num_occurences$total, " occurences of '", stem_to_look_for, "' in the given corpus.", sep = ""))
  
  num_occurences_graph
  
}


search_for_single_word_graph(corpus_relevant, "diversity")


## JIMMY NOTES 12/8
#Having spoken with my advisers, we should start in the twenty years after Brown v. Board, so 1950-1974 (stating a bit before). This  was sort of the desegregation heyday when our search terms are most likely to appear prominently, as well as the years before Brown  where a lot of the conceptual work was happening in the lower courts. 

#As a first past, I’d love to treat all the terms labeled “core” in this Excel sheet as our master dictionary (sort of like the broad dictionary they used in that Nyarko paper). In the twenty-five year period, I’d like to see the trend line for the appearance of all words in the set, treating the dictionary as a block. I’d also like to get a count and ranking of individual term appearances, and ideally a list of the cases in which they appear. I’d also love to see if they seem to be particularly pronounced in the rulings of one lower court, appellate court, or state court, and identify particular documents that seem especially saturated with dictionary terms. Going in, we know that the terms are likely to appear only in a small number of cases, so we shouldn’t be discouraged by low numbers. 

#I’d also love to get individualized results for the following words (select terms that have been already established in the literature as important/indicative of jurisprudential thinking). This list will expand and contract as the project takes further shape. Ideally, we’ll be able to pull up paragraphs with these terms for more qualitative analysis. Eventually we will want to do cosine similarities to see which terms cluster around our key words of interest, but that’s for another day! 

#Alumni 
#Associate
#Mingle 
#Dominant class
#Intangible
#Objective factor
#Social scientific 
#Mainstream
#Non-quantitative
#Old boy
#Networks
#Prestige
#Measurement
#Status 
#Exposure

search_for_single_word_graph(corpus_relevant, "exposure")

```


```{r new_function}

# need to redesign function for searching to handle multi-length words

test_phrases <- c("law", "supreme court", "united states", "jury decide whether", "status")

test_phrase_tibble <- as.tibble(test_phrases) %>%
  unnest_tokens(word, value)

corpus_40s

test_tokens <- corpus_40s %>%
  unnest_tokens(word, clean_text) %>%
  anti_join(get_stopwords())

test_tokens$rownumber = 1:nrow(test_tokens)

test_tokens_filtered <- test_tokens %>%
  right_join(test_phrase_tibble, by = "word")

final_tibble <- tibble()

# for words where n = 1

test_tokens_filtered_one_word <- test_tokens_filtered %>%
  filter(word == "law" | word == "status") %>%
  mutate(phrase = word) %>%
  select(id, slug, year, court, phrase)

final_tibble <- test_tokens_filtered_one_word

# for words where n = 2

test_tokens_filtered_two_words <- test_tokens_filtered %>%
  
  

test_tokens %>%
  filter(word == "supreme")





```



```{r multi_search}

## MULTIPLE WORDS
# this is kept for reference, but new function is being designed in previous code block


test_words <- c("diversity", "inclusion", "intangible")

raw_words <- as.tibble(test_words)

search_for_multiple_words_graph <- function(corpus_for_search, input_words){
  
  # total number of cases in the corpus
  
  num_cases = nrow(corpus_for_search)

  # unnest and get rid of common words from whole corpus
  
  tokens_graph <- corpus_for_search %>%
    unnest_tokens(word, clean_text) %>%
    anti_join(get_stopwords()) %>%
    select(word, year)
  
  # stem and count words

  token_stems_graph <- tokens_graph %>%
    group_by(year) %>%
    mutate(stem = wordStem(word)) %>%
    count(stem, sort = TRUE)
  
  # total yearly occurrences
  
  yearly_totals <- token_stems_graph %>%
    group_by(year) %>%
    summarize(yearly_total = sum(n))
  
  # turn our input word list into a tibble

  raw_words <- as.tibble(input_words)
  
  # stem those input words

  stem_words <- raw_words %>%
    mutate(stem = wordStem(value)) %>%
    select(stem)
  
  # join the table of words we want and counts of all words, leaving only the words we want

  occurences <- right_join(token_stems_graph, stem_words, by = "stem")

  # break this down by year
  
  occurences_yearly <- occurences %>%
    group_by(year) %>%
    summarize(total = sum(n)) %>%
    left_join(yearly_totals, by = "year") %>%
    mutate(percent_occurences = total/yearly_total)
  
  occurences_graph <- occurences_yearly %>%
    ggplot(aes(x = year, y = percent_occurences)) +
      geom_line() + 
      scale_x_continuous(breaks = round(seq(min(occurences_yearly$year, na.rm = TRUE), max(occurences_yearly$year, na.rm = TRUE), by = 5),1)) +
      scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
      labs(title = "Occurences of Dictory of Terms in Given Corpus",
           x = "Year", y = "Percent Occurrences", subtitle = "As a Percentage of Total Words in Corpus for a Given Year")
  
  occurences_graph
  
}

search_for_multiple_words_graph(corpus_relevant, test_words)

relevant_terms <- c("intangible", "tangible", "psychological", "prestige", "alumni", "reputation", "traditions", "aspirations",
                   "self-image", "stigma", "sociological", "diversity", "negativism", "enclave", "mingle", "conduit", "apartheid",
                   "folkways", "accreditation", "alumni", "status", "assimilation", "influence", "affluence", "stature")

search_for_multiple_words_graph(corpus_relevant, relevant_terms)


# get rankings of words

raw_tibble <- as_tibble(relevant_terms)

stem_word_rankings <- raw_tibble %>%
  mutate(stem = wordStem(value)) %>%
  mutate(word = value) %>%
  select(stem, word)

word_rankings_raw <- corpus_relevant %>%
  unnest_tokens(word, clean_text) %>%
  anti_join(get_stopwords()) %>%
  select(word) %>%
  mutate(stem = wordStem(word)) %>%
  count(stem, sort = TRUE)

word_rankings <- right_join(word_rankings_raw, stem_word_rankings, by = "stem") %>%
  select(word, stem, n)
  

# get rankings of courts

court_rankings_orig <- corpus_relevant %>%
    unnest_tokens(word, clean_text) %>%
    anti_join(get_stopwords()) %>%
    select(word, court)
  
court_rankings_raw <- court_rankings_orig %>%
  group_by(court) %>%
  mutate(stem = wordStem(word)) %>%
  count(stem, sort = TRUE)

raw_tibble_court <- as_tibble(relevant_terms)

stem_words_court <- raw_tibble_court %>%
  mutate(stem = wordStem(value)) %>%
  select(stem)

court_totals <- court_rankings_raw %>%
    group_by(court) %>%
    summarize(court_total = sum(n))

occurences_court <- right_join(court_rankings_raw, stem_words_court, by = "stem")

court_rankings <- occurences_court %>%
  group_by(court) %>%
  summarize(total_dictionary_uses = sum(n)) %>%
  left_join(court_totals, by = "court") %>%
  mutate(relative_occurences = total_dictionary_uses/court_total) %>%
  arrange(-relative_occurences)


# get rankings of cases

case_rankings_orig <- corpus_relevant %>%
    unnest_tokens(word, clean_text) %>%
    anti_join(get_stopwords()) %>%
    select(id, slug, word, court)
  
case_rankings_raw <- case_rankings_orig %>%
  group_by(slug, id) %>%
  mutate(stem = wordStem(word)) %>%
  count(stem, sort = TRUE)

raw_tibble_case <- as_tibble(relevant_terms)

stem_words_case <- raw_tibble_case %>%
  mutate(stem = wordStem(value)) %>%
  select(stem)

case_totals <- case_rankings_raw %>%
    group_by(slug, id) %>%
    summarize(case_total = sum(n))

occurences_case <- right_join(case_rankings_raw, stem_words_case, by = "stem")

case_rankings <- occurences_case %>%
  group_by(id) %>%
  summarize(total_dictionary_uses = sum(n)) %>%
  left_join(case_totals, by = "id") %>%
  mutate(relative_occurences = total_dictionary_uses/case_total) %>%
  arrange(-relative_occurences) %>%
  right_join(corpus_relevant, by = c("id", "slug")) %>%
  select(id, slug, court, total_dictionary_uses, case_total, relative_occurences)

corpus_40s



```



```{r read_data_tidy}
# This approach yielded poor results
json_file <- "school-segregation-cases/1000012.json"
json_data <- fromJSON(file=json_file)
json_data %>% spread_all
```

```{r courts}
court_counts <- data_complete %>%
  mutate(court = substr(court_raw, start = 53, stop = (nchar(court_raw)-1))) %>%
  group_by(court) %>%
  summarise(total_cases = n()) %>%
  arrange(-total_cases)
  

write_csv(court_counts, "court_counts_export.csv")

court_counts %>%
  head(20) %>%
  ggplot(aes(x = reorder(court, total_cases), y = total_cases)) +
  geom_bar(stat="identity") +
  coord_flip() +
  labs(x = "Court (abbreviated)", y = "Total Cases in Corpus", title = "Top 20 Courts Represented in Corpus")
```



