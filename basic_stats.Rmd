---
title: "basic_stats"
author: "Will Schrepferman"
date: "10/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("rjson")
library("tidyjson")
library("glue")
library("skimr")
library("openxlsx")
library("tidytext")
library("SnowballC")
```

```{r read_data_one_case}
# testing functionality of reading one single json file into a tibble row

json_file <- "school-segregation-cases/1000012.json"

# using rjson package functionality
json_data <- fromJSON(file=json_file)


# these two methods are obtional, but make the data a little uglier
# data_flat <- flatten(json_data)
# data_tbl <- as_tibble(data_flat)

# read only the elements we need from their place in the json object
full_text <- json_data$clusters[[1]]$sub_opinions[[1]]$html_lawbox[1]
id <- json_data$id
resource_uri <- json_data$resource_uri
court_raw <- json_data$court
judges <- json_data$clusters[[1]]$judges[1]
date_filed <- json_data$clusters[[1]]$date_filed[1]
slug <- json_data$clusters[[1]]$slug
federal_cite <- json_data$clusters[[1]]$federal_cite_one[1]


# put items into a tibble row
tibble_data <- tibble(id = id, slug = slug, date_filed = date_filed, court_raw = court_raw, judges = judges, federal_cite = federal_cite,
                      full_text = full_text, resource_uri = resource_uri)
```




```{r read_data_multiple_cases}
# list of all files
files <- list.files("school-segregation-cases")

# function to read one file into a coherent row
readfile <- function(file){
  
  # get the whole file
  json_file <- glue("school-segregation-cases/", file, sep = "")

  # using rjson package functionality
  json_data <- fromJSON(file=json_file)
  
  # these two methods are optional, but make the data a little uglier
  # data_flat <- flatten(json_data)
  # data_tbl <- as_tibble(data_flat)
  
  # read only the elements we need from their place in the json object
  full_text <- json_data$clusters[[1]]$sub_opinions[[1]]$html_lawbox[1]
  id <- json_data$id
  resource_uri <- json_data$resource_uri
  court_raw <- json_data$court
  judges <- json_data$clusters[[1]]$judges[1]
  date_filed <- json_data$clusters[[1]]$date_filed[1]
  slug <- json_data$clusters[[1]]$slug
  federal_cite <- json_data$clusters[[1]]$federal_cite_one[1]
  
  
  # put items into a tibble row
  tibble_data <- tibble(id = id, slug = slug, date_filed = date_filed, court_raw = court_raw, judges = judges, federal_cite = federal_cite,
                      full_text = full_text, resource_uri = resource_uri)
}

# make an empty tibble to put everything in
data_complete <- tibble()


for(i in files){
  # add tibble row from the function onto the final tibble
  data_complete <- rbind(data_complete, readfile(i))
}

tail(data_complete)

# ERROR: running the method above hits a snag when you get to file id 3032772
# will run analysis on the 8287 of 13572 objects that made their way into data frame

skim(data_complete)

head(data_complete, n = 100)

# filter out only cases for which we have all variables

none_missing_data_complete <- data_complete %>%
  filter(judges != "" & federal_cite != "" & full_text != "")

# create a set of cases with at least 1 incomplete field to export for review

incomplete_cases <- data_complete %>%
  filter(judges == "" | federal_cite == "" | full_text == "")

incomplete_cases <- incomplete_cases %>%
  mutate_if(is.character, list(~na_if(.,""))) 

skim(incomplete_cases)
head(incomplete_cases)

write.xlsx(incomplete_cases, "incomplete_cases_export.xlsx")

# 3593 of 8287 (43.4%)

# To-Do:
# Discuss which cases to exclude - DONE
# Resolve error that only gives me half of the data set - TRIED FOR AN HOUR AND COULD NOT RESOLVE
# create a 'document length' variable
# mapping court_raw to an actual court variable -> go to court listener
# export list of incomplete cases - DONE
```

```{r text_cleaning}
# create testing data with only 25 cases for computing speed purposes

test_data <- head(none_missing_data_complete, 25)

# add variables for year, clean up date type, cut fluff from court type
test_data <- test_data %>%
  mutate(year = as.numeric(substr(date_filed, start = 1, stop = 4))) %>%
  mutate(exact_date = as.Date(date_filed)) %>%
  mutate(court = substr(court_raw, start = 53, stop = (nchar(court_raw)-1))) %>%
  select(id, slug, year, court, judges, federal_cite, full_text, exact_date, resource_uri)

# get an idea of document word counts pre-clean

test_data <- test_data %>%
  mutate(doc_length_pre_clean = sapply(strsplit(full_text, " "), length))

# remove formatting characters. still working on this TODO

test_data <- test_data %>%
  mutate(clean_text = str_replace_all(full_text, "[^[:alnum:]]", " ")) %>%
  mutate(clean_text = str_remove(clean_text, "div")) %>%
  mutate(clean_text = str_remove(clean_text, "h1")) %>%
  select(id, slug, year, court, judges, federal_cite, clean_text, exact_date, resource_uri)

# unnesting tokens for one row; this will be turned into a function eventually

test_data_tokens <- test_data %>%
  filter(id == 1000012) %>%
  unnest_tokens(word, clean_text) %>%
  anti_join(get_stopwords()) %>%
  select(word)

# test for most common words. this is very broken, need to get rid of those weird characters.

test_data_tokens %>%
  count(word, sort = TRUE)

# stem words using porter stemming algorithm: https://tartarus.org/martin/PorterStemmer/

test_data_tokens_stemmed <- test_data_tokens %>%
  mutate(stem = wordStem(word)) %>%
  count(stem, sort = TRUE)

# make a lil graph

top_words_graph <- test_data_tokens_stemmed %>%
  top_n(25, n) %>%
  ggplot(aes(n, fct_reorder(stem, n))) +
  geom_col(show.legend = FALSE) +
  labs(x = "Frequency", y = NULL)

# function to do all that for any given id:

get_top_words <- function(id){
  test_data_tokens <- test_data %>%
    filter(id == id) %>%
    unnest_tokens(word, clean_text) %>%
    anti_join(get_stopwords()) %>%
    select(word)

  test_data_tokens %>%
    count(word, sort = TRUE)
  
  test_data_tokens_stemmed <- test_data_tokens %>%
    mutate(stem = wordStem(word)) %>%
    count(stem, sort = TRUE)
  
  top_words_graph <- test_data_tokens_stemmed %>%
    top_n(25, n) %>%
    ggplot(aes(n, fct_reorder(stem, n))) +
    geom_col(show.legend = FALSE) +
    labs(x = "Frequency", y = NULL)
  
  (top_words_graph)
}

get_top_words(id = 1002370)

```



```{r read_data_tidy}
# This approach yielded poor results
json_file <- "school-segregation-cases/1000012.json"
json_data <- fromJSON(file=json_file)
json_data %>% spread_all
```


