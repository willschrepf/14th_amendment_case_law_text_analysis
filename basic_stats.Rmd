---
title: "basic_stats"
author: "Will Schrepferman"
date: "10/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("tidyverse")
library("rjson")
library("tidyjson")
library("glue")
library("skimr")
library("openxlsx")
library("tidytext")
library("SnowballC")
library("rvest")
```

```{r read_data_one_case}
# testing functionality of reading one single json file into a tibble row

json_file <- "school-segregation-cases/1000012.json"

# using rjson package functionality
json_data <- fromJSON(file=json_file)


# these two methods are obtional, but make the data a little uglier
# data_flat <- flatten(json_data)
# data_tbl <- as_tibble(data_flat)

# read only the elements we need from their place in the json object
full_text <- json_data$clusters[[1]]$sub_opinions[[1]]$html_lawbox[1]
id <- json_data$id
resource_uri <- json_data$resource_uri
court_raw <- json_data$court
judges <- json_data$clusters[[1]]$judges[1]
date_filed <- json_data$clusters[[1]]$date_filed[1]
slug <- json_data$clusters[[1]]$slug
federal_cite <- json_data$clusters[[1]]$federal_cite_one[1]


# put items into a tibble row
tibble_data <- tibble(id = id, slug = slug, date_filed = date_filed, court_raw = court_raw, judges = judges, federal_cite = federal_cite,
                      full_text = full_text, resource_uri = resource_uri)
```




```{r read_data_multiple_cases}
# list of all files
files <- list.files("school-segregation-cases")

# function to read one file into a coherent row
readfile <- function(file){
  
  # get the whole file
  json_file <- glue("school-segregation-cases/", file, sep = "")

  # using rjson package functionality
  json_data <- fromJSON(file=json_file)
  
  # these two methods are optional, but make the data a little uglier
  # data_flat <- flatten(json_data)
  # data_tbl <- as_tibble(data_flat)
  
  # read only the elements we need from their place in the json object
  full_text <- json_data$clusters[[1]]$sub_opinions[[1]]$html_lawbox[1]
  id <- json_data$id
  resource_uri <- json_data$resource_uri
  court_raw <- json_data$court
  judges <- json_data$clusters[[1]]$judges[1]
  date_filed <- json_data$clusters[[1]]$date_filed[1]
  slug <- json_data$clusters[[1]]$slug
  federal_cite <- json_data$clusters[[1]]$federal_cite_one[1]
  
  
  # put items into a tibble row
  tibble_data <- tibble(id = id, slug = slug, date_filed = date_filed, court_raw = court_raw, judges = judges, federal_cite = federal_cite,
                      full_text = full_text, resource_uri = resource_uri)
}

# make an empty tibble to put everything in
data_complete <- tibble()


for(i in files){
  # add tibble row from the function onto the final tibble
  data_complete <- rbind(data_complete, readfile(i))
}

tail(data_complete)

# ERROR: running the method above hits a snag when you get to file id 3032772
# will run analysis on the 8287 of 13572 objects that made their way into data frame

skim(data_complete)

head(data_complete, n = 100)

# filter out only cases for which we have all variables

none_missing_data_complete <- data_complete %>%
  filter(judges != "" & federal_cite != "" & full_text != "")

# create a set of cases with at least 1 incomplete field to export for review

incomplete_cases <- data_complete %>%
  filter(judges == "" | federal_cite == "" | full_text == "")

incomplete_cases <- incomplete_cases %>%
  mutate_if(is.character, list(~na_if(.,""))) 

skim(incomplete_cases)
head(incomplete_cases)

write.xlsx(incomplete_cases, "incomplete_cases_export.xlsx")

# 3593 of 8287 (43.4%)

# To-Do:
# Discuss which cases to exclude - DONE
# Resolve error that only gives me half of the data set - TRIED FOR AN HOUR AND COULD NOT RESOLVE
# create a 'document length' variable - DONE
# mapping court_raw to an actual court variable -> go to court listener
# export list of incomplete cases - DONE
# begin stemming + preprocessing text - DONE
```

```{r text_cleaning}
# create testing data with only 25 cases for computing speed purposes

test_data <- head(none_missing_data_complete, 25)

# add variables for year, clean up date type, cut fluff from court type
test_data <- test_data %>%
  mutate(year = as.numeric(substr(date_filed, start = 1, stop = 4))) %>%
  mutate(exact_date = as.Date(date_filed)) %>%
  mutate(court = substr(court_raw, start = 53, stop = (nchar(court_raw)-1))) %>%
  select(id, slug, year, court, judges, federal_cite, full_text, exact_date, resource_uri)

# get an idea of document word counts pre-clean

test_data <- test_data %>%
  mutate(doc_length_pre_clean = sapply(strsplit(full_text, " "), length))

# remove formatting characters. still working on this TODO

#test_data <- test_data %>%
#  mutate(clean_text = str_replace_all(full_text, "[^[:alnum:]]", " ")) %>%
#  mutate(clean_text = str_remove(clean_text, "div")) %>%
#  mutate(clean_text = str_remove(clean_text, "h1")) %>%
#  select(id, slug, year, court, judges, federal_cite, clean_text, exact_date, resource_uri)

# update, it all worked with this silly goose of a function:

strip_html <- function(s) {
    html_text(read_html(s))
}

# ANTI JOIN REFERENCE TERMS when jimmy sends them

clean_text <- function(htmlString) {
  return(gsub("<.*?>", "", htmlString))
}

test_data <- test_data %>%
  mutate(clean_text = clean_text(full_text)) %>%
  mutate(doc_length_post_clean = sapply(strsplit(clean_text, " "), length))

# unnesting tokens for one row; this will be turned into a function eventually

test_data_tokens <- test_data %>%
  filter(id == 1000012) %>%
  unnest_tokens(word, clean_text) %>%
  anti_join(get_stopwords()) %>%
  select(word)

# test for most common words. this is very broken, need to get rid of those weird characters.

test_data_tokens %>%
  count(word, sort = TRUE)

# stem words using porter stemming algorithm: https://tartarus.org/martin/PorterStemmer/

test_data_tokens_stemmed <- test_data_tokens %>%
  mutate(stem = wordStem(word)) %>%
  count(stem, sort = TRUE)

# make a lil graph

top_words_graph <- test_data_tokens_stemmed %>%
  top_n(25, n) %>%
  ggplot(aes(n, fct_reorder(stem, n))) +
  geom_col(show.legend = FALSE) +
  labs(x = "Frequency", y = NULL)

# function to do all that for any given id:

get_top_words <- function(id){
  test_data_tokens <- test_data %>%
    filter(id == id) %>%
    unnest_tokens(word, clean_text) %>%
    anti_join(get_stopwords()) %>%
    select(word)

  test_data_tokens %>%
    count(word, sort = TRUE)
  
  test_data_tokens_stemmed <- test_data_tokens %>%
    mutate(stem = wordStem(word)) %>%
    count(stem, sort = TRUE)
  
  top_words_graph <- test_data_tokens_stemmed %>%
    top_n(40, n) %>%
    ggplot(aes(n, fct_reorder(stem, n))) +
    geom_col(show.legend = FALSE) +
    labs(x = "Frequency", y = NULL)
  
  (top_words_graph)
}

get_top_words(id = 1003718)



# BEGIN WORKING ON WORD FREQUENCY SEARCH



```


```{r single_search}
full_corpus <- none_missing_data_complete %>%
  mutate(year = as.numeric(substr(date_filed, start = 1, stop = 4))) %>%
  mutate(exact_date = as.Date(date_filed)) %>%
  mutate(court = substr(court_raw, start = 53, stop = (nchar(court_raw)-1))) %>%
  select(id, slug, year, court, judges, federal_cite, full_text, exact_date, resource_uri)

corpus_40s <- full_corpus %>%
  filter(year >= 1940 & year <= 1949) %>%
  mutate(doc_length_pre_clean = sapply(strsplit(full_text, " "), length)) %>%
  mutate(clean_text = clean_text(full_text)) %>%
  mutate(doc_length_post_clean = sapply(strsplit(clean_text, " "), length)) %>%
  select(id, slug, year, court, judges, federal_cite, clean_text, doc_length_post_clean, exact_date, resource_uri)

corpus_relevant <- full_corpus %>%
  filter(year >= 1950 & year <= 1974) %>%
  mutate(doc_length_pre_clean = sapply(strsplit(full_text, " "), length)) %>%
  mutate(clean_text = clean_text(full_text)) %>%
  mutate(doc_length_post_clean = sapply(strsplit(clean_text, " "), length)) %>%
  select(id, slug, year, court, judges, federal_cite, clean_text, doc_length_post_clean, exact_date, resource_uri)


search_for_single_word <- function(corpus_for_search, input_word){
  
  num_cases = nrow(corpus_for_search)
  
  tokens <- corpus_for_search %>%
    unnest_tokens(word, clean_text) %>%
    anti_join(get_stopwords()) %>%
    select(word)
  
  token_stems <- tokens %>%
    mutate(stem = wordStem(word)) %>%
    count(stem, sort = TRUE)
  
  stem_input_word <- wordStem(input_word)
  
  num_occurences <- token_stems %>%
    filter(stem == stem_input_word) %>%
    select(n) %>%
    slice(1) %>%
    pull(1)
  
  num_occurences
}

search_for_single_word(corpus_relevant, "diversity")

search_for_single_word_graph <- function(corpus_for_search, input_word){
  
  tokens_graph <- corpus_for_search %>%
    unnest_tokens(word, clean_text) %>%
    anti_join(get_stopwords()) %>%
    select(word, year)
  
  token_stems_graph <- tokens_graph %>%
    group_by(year) %>%
    mutate(stem = wordStem(word)) %>%
    count(stem, sort = TRUE)
  
  yearly_totals <- token_stems_graph %>%
    group_by(year) %>%
    summarize(yearly_total = sum(n))
  
  stem_input_word_graph <- wordStem(input_word)
  
  num_occurences_tibble <- token_stems_graph %>%
    filter(stem == stem_input_word_graph)
  
  total_num_occurences <- num_occurences_tibble %>%
    ungroup() %>%
    summarise(total = sum(n))
  
  num_occurences_data <- right_join(num_occurences_tibble, yearly_totals, by = "year") %>%
    mutate(percent_occurences = (n/yearly_total)) %>%
    select(stem, year, percent_occurences) %>%
    arrange(year)
  
  num_occurences_data <- replace_na(num_occurences_data, list(percent_occurences = 0, stem = stem_input_word_graph))
  
  stem_to_look_for_array <- num_occurences_data %>%
    select(stem) %>%
    pull(2)
  
  stem_to_look_for <- stem_to_look_for_array[1]
    
  num_occurences_graph <- num_occurences_data %>%
    ggplot(aes(x = year, y = percent_occurences)) +
    geom_line() + 
    scale_x_continuous(breaks = round(seq(min(yearly_totals$year), max(yearly_totals$year), by = 5),1)) +
    scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
    labs(title = paste("Occurences of Stem: '", stem_to_look_for, "' (lemmatized from '", input_word, "') in Given Corpus", sep = ""),
         x = "Year", y = "Percent Occurrences", 
         subtitle = "As a Percentage of Total Words in Corpus for a Given Year",
         caption = paste("There were ", total_num_occurences$total, " occurences of '", stem_to_look_for, "' in the given corpus.", sep = ""))
  
  num_occurences_graph
  
}


search_for_single_word_graph(corpus_relevant, "diversity")


## JIMMY NOTES 12/8
#Having spoken with my advisers, we should start in the twenty years after Brown v. Board, so 1950-1974 (stating a bit before). This  was sort of the desegregation heyday when our search terms are most likely to appear prominently, as well as the years before Brown  where a lot of the conceptual work was happening in the lower courts. 

#As a first past, I’d love to treat all the terms labeled “core” in this Excel sheet as our master dictionary (sort of like the broad dictionary they used in that Nyarko paper). In the twenty-five year period, I’d like to see the trend line for the appearance of all words in the set, treating the dictionary as a block. I’d also like to get a count and ranking of individual term appearances, and ideally a list of the cases in which they appear. I’d also love to see if they seem to be particularly pronounced in the rulings of one lower court, appellate court, or state court, and identify particular documents that seem especially saturated with dictionary terms. Going in, we know that the terms are likely to appear only in a small number of cases, so we shouldn’t be discouraged by low numbers. 

#I’d also love to get individualized results for the following words (select terms that have been already established in the literature as important/indicative of jurisprudential thinking). This list will expand and contract as the project takes further shape. Ideally, we’ll be able to pull up paragraphs with these terms for more qualitative analysis. Eventually we will want to do cosine similarities to see which terms cluster around our key words of interest, but that’s for another day! 

#Alumni 
#Associate
#Mingle 
#Dominant class
#Intangible
#Objective factor
#Social scientific 
#Mainstream
#Non-quantitative
#Old boy
#Networks
#Prestige
#Measurement
#Status 
#Exposure

search_for_single_word_graph(corpus_relevant, "exposure")

```

```{r multi_search}

## MULTIPLE WORDS

search_for_multiple_words <- function(corpus_for_search, input_words){
  
  num_cases = nrow(corpus_for_search)

  tokens <- corpus_for_search %>%
    unnest_tokens(word, clean_text) %>%
    anti_join(get_stopwords()) %>%
    select(word)

  token_stems <- tokens %>%
    mutate(stem = wordStem(word)) %>%
    count(stem, sort = TRUE)

  raw_words <- as.tibble(input_words)

  stem_words <- raw_words %>%
    mutate(stem = wordStem(value)) %>%
    select(stem)

  occurences <- right_join(token_stems, stem_words, by = "stem")

  occurences
}


test_words <- c("diversity", "inclusion", "intangible")

raw_words <- as.tibble(test_words)


search_for_multiple_words(corpus_40s, test_words)


search_for_multiple_words_graph <- function(corpus_for_search, input_words){
  num_cases = nrow(corpus_for_search)

  tokens_graph <- corpus_for_search %>%
    unnest_tokens(word, clean_text) %>%
    anti_join(get_stopwords()) %>%
    select(word, year)

  token_stems_graph <- tokens_graph %>%
    group_by(year) %>%
    mutate(stem = wordStem(word)) %>%
    count(stem, sort = TRUE)
  
  yearly_totals <- token_stems_graph %>%
    group_by(year) %>%
    summarize(yearly_total = sum(n))

  raw_words <- as.tibble(input_words)

  stem_words <- raw_words %>%
    mutate(stem = wordStem(value)) %>%
    select(stem)

  occurences <- right_join(token_stems_graph, stem_words, by = "stem")

  occurences_yearly <- occurences %>%
    group_by(year) %>%
    summarize(total = sum(n)) %>%
    left_join(yearly_totals, by = "year") %>%
    mutate(percent_occurences = total/yearly_total)
  
  occurences_graph <- occurences_yearly %>%
    ggplot(aes(x = year, y = percent_occurences)) +
      geom_line() + 
      scale_x_continuous(breaks = round(seq(min(occurences_yearly$year), max(occurences_yearly$year), by = 5),1)) +
      scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
      labs(title = "Occurences of Set of Given Terms in Given Corpus",
           x = "Year", y = "Percent Occurrences", subtitle = "As a Percentage of Total Words in Corpus for a Given Year")
  
  occurences_graph
  
}


search_for_multiple_words_graph(corpus_relevant, test_words)

```



```{r read_data_tidy}
# This approach yielded poor results
json_file <- "school-segregation-cases/1000012.json"
json_data <- fromJSON(file=json_file)
json_data %>% spread_all
```

```{r courts}
court_counts <- data_complete %>%
  mutate(court = substr(court_raw, start = 53, stop = (nchar(court_raw)-1))) %>%
  group_by(court) %>%
  summarise(total_cases = n()) %>%
  arrange(-total_cases)
  

write_csv(court_counts, "court_counts_export.csv")

court_counts %>%
  head(20) %>%
  ggplot(aes(x = reorder(court, total_cases), y = total_cases)) +
  geom_bar(stat="identity") +
  coord_flip() +
  labs(x = "Court (abbreviated)", y = "Total Cases in Corpus", title = "Top 20 Courts Represented in Corpus")
```



